"""
Grader Store - Single source of truth for pick persistence and autograder reads.

CRITICAL: This is the ONLY module that writes/reads predictions for autograding.
All picks generated by /live/best-bets/* must persist here.
Autograder reads from here to grade tomorrow.

Storage: Railway volume at /data/grader
Format: JSONL (one pick per line) for atomic appends
"""

import json
import os
import fcntl
from datetime import datetime, timezone
from typing import Dict, Any, List, Optional
from pathlib import Path
import logging

logger = logging.getLogger(__name__)

# Import storage_paths - SINGLE SOURCE OF TRUTH for Railway volume
from storage_paths import get_store_dir, get_predictions_file, get_weights_file, get_audit_dir

# =============================================================================
# SINGLE SOURCE OF TRUTH: All paths from storage_paths.py (Railway volume)
# =============================================================================
STORAGE_ROOT = get_store_dir()
PREDICTIONS_FILE = get_predictions_file()
WEIGHTS_FILE = get_weights_file()
AUDIT_DIR = get_audit_dir()


def ensure_storage_writable():
    """
    Verify storage directory exists and is writable.
    FAIL FAST on startup if not writable.
    """
    try:
        # Create all required directories
        Path(STORAGE_ROOT).mkdir(parents=True, exist_ok=True)
        Path(AUDIT_DIR).mkdir(parents=True, exist_ok=True)

        # Test write
        test_file = os.path.join(STORAGE_ROOT, ".write_test")
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)

        # Log storage info
        exists = os.path.exists(PREDICTIONS_FILE)
        size_bytes = os.path.getsize(PREDICTIONS_FILE) if exists else 0

        logger.info("GRADER_STORE: storage_root=%s predictions_file=%s exists=%s writable=True bytes=%d",
                    STORAGE_ROOT, PREDICTIONS_FILE, exists, size_bytes)

        return True

    except Exception as e:
        logger.error("GRADER_STORE: Storage not writable at %s: %s", STORAGE_ROOT, e)
        raise RuntimeError(f"Storage not writable: {e}")


def _make_pick_id(pick: Dict[str, Any]) -> str:
    """
    Generate stable pick_id hash.

    Format: sport+event_id+market+player_id/side+line+book+date_et
    """
    import hashlib

    sport = pick.get("sport", "").upper()
    event_id = pick.get("event_id", "")
    market = pick.get("market", pick.get("pick_type", "")).upper()

    # For props: use player_id; for games: use side/team
    player_id = pick.get("canonical_player_id", "")
    side = pick.get("side", "")
    subject = player_id if player_id else side

    line = pick.get("line", 0)
    book = pick.get("book_key", "consensus")
    date_et = pick.get("date_et", "")

    key = f"{sport}|{event_id}|{market}|{subject}|{line:.1f}|{book}|{date_et}"
    return hashlib.sha1(key.encode()).hexdigest()[:12]


def persist_pick(pick: Dict[str, Any], date_et: str) -> bool:
    """
    Persist a single pick to grader store (idempotent by pick_id).

    Args:
        pick: Pick dictionary with all required fields
        date_et: Today's ET date (YYYY-MM-DD)

    Returns:
        True if persisted, False if duplicate
    """
    try:
        # Generate pick_id BEFORE adding persisted_at (for consistent hashing)
        pick["date_et"] = date_et
        pick_id = _make_pick_id(pick)
        pick["pick_id"] = pick_id
        pick["persisted_at"] = datetime.now(timezone.utc).isoformat()

        # Load existing picks to check for duplicates
        existing_ids = set()
        if os.path.exists(PREDICTIONS_FILE):
            with open(PREDICTIONS_FILE, 'r') as f:
                fcntl.flock(f.fileno(), fcntl.LOCK_SH)
                try:
                    for line in f:
                        if line.strip():
                            existing = json.loads(line)
                            existing_ids.add(existing.get("pick_id"))
                finally:
                    fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        # Skip if duplicate
        if pick_id in existing_ids:
            return False

        # Append pick (atomic with file lock)
        with open(PREDICTIONS_FILE, 'a') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                f.write(json.dumps(pick) + "\n")
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        return True

    except Exception as e:
        logger.exception("Failed to persist pick: %s", e)
        return False


def persist_picks_batch(picks: List[Dict[str, Any]], date_et: str) -> Dict[str, int]:
    """
    Persist multiple picks (idempotent).

    Returns:
        {"persisted": N, "duplicates": M}
    """
    persisted = 0
    duplicates = 0

    for pick in picks:
        if persist_pick(pick, date_et):
            persisted += 1
        else:
            duplicates += 1

    return {"persisted": persisted, "duplicates": duplicates}


def load_predictions(date_et: Optional[str] = None) -> List[Dict[str, Any]]:
    """
    Load all predictions from store, optionally filtered by date.

    Supports both JSONL (one JSON object per line) and legacy JSON array format.

    Args:
        date_et: Filter to specific ET date (YYYY-MM-DD), or None for all

    Returns:
        List of prediction dicts
    """
    predictions = []

    if not os.path.exists(PREDICTIONS_FILE):
        return predictions

    try:
        with open(PREDICTIONS_FILE, 'r') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_SH)
            try:
                # Read entire file content
                content = f.read()

                # If empty, return empty list
                if not content.strip():
                    return predictions

                # Check if legacy JSON array format
                if content.lstrip().startswith('['):
                    # Parse as JSON array (legacy support)
                    try:
                        all_preds = json.loads(content)
                        if isinstance(all_preds, list):
                            predictions = all_preds
                    except json.JSONDecodeError as e:
                        logger.error("Failed to parse legacy JSON array: %s", e)
                        return predictions
                else:
                    # Parse as JSONL (one JSON object per line)
                    for line_num, line in enumerate(content.splitlines(), 1):
                        line = line.strip()
                        if not line:
                            continue

                        try:
                            pred = json.loads(line)
                            # Validate it's a dict with pick_id
                            if isinstance(pred, dict) and "pick_id" in pred:
                                predictions.append(pred)
                            else:
                                logger.warning("Line %d: Not a valid pick dict (missing pick_id)", line_num)
                        except json.JSONDecodeError as e:
                            logger.error("Line %d: Failed to parse JSON: %s", line_num, e)
                            # Skip corrupted line, continue processing

                # Filter by date if specified
                if date_et is not None:
                    predictions = [p for p in predictions if p.get("date_et") == date_et]

            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.exception("Failed to load predictions: %s", e)

    return predictions


def load_predictions_with_reconciliation() -> Dict[str, Any]:
    """
    Load predictions with full reconciliation stats.

    Returns dict with:
        - predictions: List of valid predictions
        - reconciliation: {
            total_lines: int,
            parsed_ok: int,
            skipped_total: int,
            skip_reasons: {reason: count},
            reconciled: bool (total_lines == parsed_ok + skipped_total)
        }
    """
    predictions = []
    skip_reasons = {
        "empty_line": 0,
        "json_parse_error": 0,
        "not_dict": 0,
        "missing_pick_id": 0,
        "legacy_array": 0,
    }
    total_lines = 0
    parsed_ok = 0

    if not os.path.exists(PREDICTIONS_FILE):
        return {
            "predictions": [],
            "reconciliation": {
                "total_lines": 0,
                "parsed_ok": 0,
                "skipped_total": 0,
                "skip_reasons": skip_reasons,
                "reconciled": True,
            }
        }

    try:
        with open(PREDICTIONS_FILE, 'r') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_SH)
            try:
                content = f.read()

                if not content.strip():
                    return {
                        "predictions": [],
                        "reconciliation": {
                            "total_lines": 0,
                            "parsed_ok": 0,
                            "skipped_total": 0,
                            "skip_reasons": skip_reasons,
                            "reconciled": True,
                        }
                    }

                # Check if legacy JSON array format
                if content.lstrip().startswith('['):
                    try:
                        all_preds = json.loads(content)
                        if isinstance(all_preds, list):
                            predictions = [p for p in all_preds if isinstance(p, dict) and "pick_id" in p]
                            parsed_ok = len(predictions)
                            total_lines = len(all_preds)
                            skip_reasons["legacy_array"] = total_lines - parsed_ok
                    except json.JSONDecodeError:
                        skip_reasons["json_parse_error"] = 1
                        total_lines = 1
                else:
                    # Parse as JSONL
                    for line in content.splitlines():
                        total_lines += 1
                        line = line.strip()

                        if not line:
                            skip_reasons["empty_line"] += 1
                            continue

                        try:
                            pred = json.loads(line)
                            if not isinstance(pred, dict):
                                skip_reasons["not_dict"] += 1
                            elif "pick_id" not in pred:
                                skip_reasons["missing_pick_id"] += 1
                            else:
                                predictions.append(pred)
                                parsed_ok += 1
                        except json.JSONDecodeError:
                            skip_reasons["json_parse_error"] += 1

            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)
    except Exception as e:
        logger.exception("Failed to load predictions with reconciliation: %s", e)

    skipped_total = sum(skip_reasons.values())
    reconciled = (total_lines == parsed_ok + skipped_total)

    # Sort skip reasons by count descending, filter zeros
    top_skip_reasons = {k: v for k, v in sorted(skip_reasons.items(), key=lambda x: -x[1]) if v > 0}

    return {
        "predictions": predictions,
        "reconciliation": {
            "total_lines": total_lines,
            "parsed_ok": parsed_ok,
            "skipped_total": skipped_total,
            "skip_reasons": top_skip_reasons,
            "reconciled": reconciled,
        }
    }


def get_storage_stats() -> Dict[str, Any]:
    """
    Get storage statistics for health checks.

    Returns:
        Dict with detailed storage info including:
        - predictions_file (absolute path)
        - predictions_file_exists
        - predictions_file_size_bytes
        - predictions_loaded_count
        - cwd
        - storage_root
        - container_hostname
    """
    import socket

    exists = os.path.exists(PREDICTIONS_FILE)
    cwd = os.getcwd()
    hostname = socket.gethostname()

    if not exists:
        return {
            "predictions_file": PREDICTIONS_FILE,
            "predictions_file_exists": False,
            "predictions_file_size_bytes": 0,
            "predictions_loaded_count": 0,
            "cwd": cwd,
            "storage_root": STORAGE_ROOT,
            "storage_root_writable": os.access(STORAGE_ROOT, os.W_OK) if os.path.exists(STORAGE_ROOT) else False,
            "container_hostname": hostname
        }

    try:
        size_bytes = os.path.getsize(PREDICTIONS_FILE)
        predictions = load_predictions()

        return {
            "predictions_file": PREDICTIONS_FILE,
            "predictions_file_exists": True,
            "predictions_file_size_bytes": size_bytes,
            "predictions_loaded_count": len(predictions),
            "cwd": cwd,
            "storage_root": STORAGE_ROOT,
            "storage_root_writable": os.access(STORAGE_ROOT, os.W_OK),
            "container_hostname": hostname
        }
    except Exception as e:
        logger.exception("Failed to get storage stats: %s", e)
        return {
            "predictions_file": PREDICTIONS_FILE,
            "predictions_file_exists": exists,
            "predictions_file_size_bytes": 0,
            "predictions_loaded_count": 0,
            "cwd": cwd,
            "storage_root": STORAGE_ROOT,
            "storage_root_writable": False,
            "container_hostname": hostname,
            "error": str(e)
        }


def mark_graded(pick_id: str, result: str, actual_value: float, graded_at: str) -> bool:
    """
    Mark a prediction as graded (update in place).

    Args:
        pick_id: Pick identifier
        result: WIN/LOSS/PUSH
        actual_value: Actual stat value
        graded_at: ISO timestamp

    Returns:
        True if updated, False if not found
    """
    if not os.path.exists(PREDICTIONS_FILE):
        return False

    try:
        # Read all predictions
        predictions = []
        updated = False

        with open(PREDICTIONS_FILE, 'r') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_SH)
            try:
                for line in f:
                    if line.strip():
                        pred = json.loads(line)
                        if pred.get("pick_id") == pick_id:
                            pred["grade_status"] = "GRADED"
                            pred["result"] = result
                            pred["actual_value"] = actual_value
                            pred["graded_at"] = graded_at
                            updated = True
                        predictions.append(pred)
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        if not updated:
            return False

        # Write back atomically
        with open(PREDICTIONS_FILE, 'w') as f:
            fcntl.flock(f.fileno(), fcntl.LOCK_EX)
            try:
                for pred in predictions:
                    f.write(json.dumps(pred) + "\n")
            finally:
                fcntl.flock(f.fileno(), fcntl.LOCK_UN)

        return True

    except Exception as e:
        logger.exception("Failed to mark pick graded: %s", e)
        return False


def selfcheck_write_read() -> Dict[str, Any]:
    """
    Self-check: Write a synthetic prediction and immediately read it back.

    Returns:
        Dict with:
        - pick_id_written: The synthetic pick_id
        - write_success: bool
        - read_success: bool
        - found_in_loaded: bool
        - predictions_before: count before write
        - predictions_after: count after write
        - file_size_before: bytes before write
        - file_size_after: bytes after write
    """
    import uuid

    # Generate unique pick_id
    pick_id = f"selfcheck-{uuid.uuid4().hex[:8]}"

    # Get counts before
    size_before = os.path.getsize(PREDICTIONS_FILE) if os.path.exists(PREDICTIONS_FILE) else 0
    preds_before = load_predictions()
    count_before = len(preds_before)

    # Write synthetic prediction
    synthetic_pick = {
        "pick_id": pick_id,
        "sport": "SELFCHECK",
        "event_id": "selfcheck",
        "market": "SELFCHECK",
        "side": "Test",
        "line": 0,
        "book_key": "test",
        "final_score": 10.0,
        "date_et": "2026-01-28",
        "grade_status": "PENDING"
    }

    write_success = persist_pick(synthetic_pick, "2026-01-28")

    # Get counts after
    size_after = os.path.getsize(PREDICTIONS_FILE) if os.path.exists(PREDICTIONS_FILE) else 0
    preds_after = load_predictions()
    count_after = len(preds_after)

    # Check if our pick_id is in loaded predictions
    found = any(p.get("pick_id") == pick_id for p in preds_after)

    return {
        "pick_id_written": pick_id,
        "write_success": write_success,
        "read_success": count_after > count_before or found,
        "found_in_loaded": found,
        "predictions_before": count_before,
        "predictions_after": count_after,
        "file_size_before": size_before,
        "file_size_after": size_after
    }
